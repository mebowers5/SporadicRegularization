---
title: "How to fully disclose"
format: html
editor: visual
---

## Objective

This vignette is a step by step guide to the methods in "Full Disclosure:...". It will show you how to simulate semi-random tracks, derive acoustic telemetry data from those tracks, model/reconstruct tracks from those derived detection data, re-route them off land, summarize distribution information, and compare the simulated and modeled sumarizations. These comparisons represent the statistical bias incurred by the modeling reconstruction and are directly related to the orientation of receivers within the study area.

Let's begin by clearing our environment.

```{r}
#| label: Clear-environment
#| include: true

rm(list=ls())
```

And clearing any cache we may have remaining from previous work.

```{r}
#| label: Clear-cache
#| include: true

gc()
```

Next, let's load the packages that we will need to run the steps in this vignette.

```{r}
#| label: load-packages
#| include: true

library(dplyr)
library(sf)
library(sp)
library(pathroutr)
library(parallel)
library(geosphere)
library(mapview)
library(oceanmap)
```

## Get the functions required for this vignette - replace with package download when available

We've customized some functions for this workflow so let's make sure we have those.

```{r}
#| label: get-functions
#| include: true

#Create function to make line from each pair of coordinates
make_line <- function(start_x, start_y, end_x, end_y) {
  sf::st_linestring(matrix(c(start_x, end_x, start_y, end_y), 2, 2))
}

grid_res <- function(km, study_site, epsg, what) {
  
  grid_spacing <- km * 1000 # Desired kilometers times 1000 m per 1 km
  
  ifelse(what == "polygons", {
  
  grid <- sf::st_make_grid(study_site, square = T, what = what, cellsize = c(grid_spacing, grid_spacing)) %>% # Create a grid inside the coastal 500 m isobath polygon
    sf::st_as_sf() %>%
    mutate(gid = seq_along(geometry)) %>%
    sf::st_make_valid() %>%
    sf::st_transform(epsg) %>%
    sf::st_cast('MULTIPOLYGON') 
  
  grid <- sf::st_intersection(study_site %>% sf::st_transform(epsg), grid) %>%
    sf::st_as_sf() %>%
    mutate(gid = seq_along(geometry)) %>%
    sf::st_make_valid() %>%
    sf::st_transform(epsg) %>%
    sf::st_cast('MULTIPOLYGON')
  
  return(grid)
  }, ifelse(what == "centers", 
  {
    grid <- sf::st_make_grid(study_site, square = T, what = what, cellsize = c(grid_spacing, grid_spacing)) %>% # Create a grid inside the coastal 500 m isobath polygon
      sf::st_as_sf() %>%
      mutate(gid = seq_along(geometry)) %>%
      sf::st_make_valid() %>%
      sf::st_transform(epsg) %>%
      sf::st_cast('POINT') 
    
    return(grid)
  }, 
  print("Error: Not a valid argument for parameter 'what'")
  ))
}

# Simulate tracks
simul_trks <- function(anims, study_site, theta, vmin, vmax, rel_site, n_days, initHeading){
  library(sp)
  
  sim <- base::replicate(n = anims, expr = {
    
    # Clear cache
    gc()
    
    initPos <- sf::st_sample(rel_site$geometry, size=1, type ="random", exact = TRUE) %>%
      as.data.frame() %>%
      mutate(lon = sf::st_coordinates(geometry)[,1],
             lat = sf::st_coordinates(geometry)[,2]) 
    
    stepLen <- as.numeric(sample(vmin:vmax, 1)*60*60*24/24) # Sample between minimum and maximum velocity of blacktip sharks to set step length per hour for each individual
    
    simu <- glatos::crw_in_polygon(study_site, 
                           theta = theta, 
                           stepLen = stepLen,
                             #as.numeric(sample(vmin:vmax, 1)*60*60*24/24), # sample between min and max animal velocity in m/s * 60 seconds/min * 60 minutes/hour * 24 hours/day
                           initPos = c(initPos$lon, initPos$lat), 
                           nsteps = n_days*24, initHeading = initHeading) %>% 
      sf::st_as_sf()
    
    return(simu)
  }
  )
  
  simu <- sim %>% as.data.frame() %>% 
    setNames(gsub("geometry.", perl = TRUE, "", names(.))) %>%
    setNames(gsub("geometry", perl = TRUE, "0", names(.))) %>%
    tidyr::pivot_longer(., cols = everything(), names_to = "AnimalID", values_to = "geom") %>% 
    mutate(ID = as.numeric(AnimalID) + 1) %>%
    group_by(ID) %>%
    mutate(uid = seq_along(AnimalID)) #Assign sequential numbers to rows to preserve directionality
  
  simu <- simu %>%  
    mutate(POINT_X = sf::st_coordinates(geom)[,1], POINT_Y = sf::st_coordinates(geom)[,2]) %>%  #Obtain x and y coordinates so that you can force northward and southward movement
    dplyr::select(uid, 
                  ID, 
                  geom, POINT_Y, POINT_X) %>%
    group_by(ID) %>%
    arrange(uid) %>%
    mutate(time = seq.POSIXt(from = as.POSIXct("2000-01-01 01:00:00"), by = "1 hour", along.with = uid),   #Make up dates for the row numbers by Iteration and Animal ID
           start_x = POINT_X, start_y = POINT_Y, end_x = lead(POINT_X), end_y = lead(POINT_Y)) %>% #Prep data for making lines - ensures proper order
    sf::st_as_sf()
  
  #Make lines and unite geometries by uid's so that each line segment maintains time information
  simu <- simu %>%
    filter(!is.na(end_y)) %>%
    tidyr::nest() %>% 
    mutate(
      data = purrr::map(data, 
                        ~ mutate(.x,
                                 x = purrr::pmap(.l = list(start_x, start_y, end_x, end_y),
                                                 .f = make_line)))) 
  
  return(simu)
}

# Simulate tracks
simul_trks <- function(anims, study_site, theta, vmin, vmax, rel_site, n_days, initHeading){
  library(sp)
  
  sim <- base::replicate(n = anims, expr = {
    
    # Clear cache
    gc()
    
    initPos <- sf::st_sample(rel_site$geometry, size=1, type ="random", exact = TRUE) %>%
      as.data.frame() %>%
      mutate(lon = sf::st_coordinates(geometry)[,1],
             lat = sf::st_coordinates(geometry)[,2]) 
    
    stepLen <- as.numeric(sample(vmin:vmax, 1)*60*60*24/24) # Sample between minimum and maximum velocity of blacktip sharks to set step length per hour for each individual
    
    simu <- glatos::crw_in_polygon(study_site, 
                           theta = theta, 
                           stepLen = stepLen,
                             #as.numeric(sample(vmin:vmax, 1)*60*60*24/24), # sample between min and max animal velocity in m/s * 60 seconds/min * 60 minutes/hour * 24 hours/day
                           initPos = c(initPos$lon, initPos$lat), 
                           nsteps = n_days*24, initHeading = initHeading) %>% 
      sf::st_as_sf()
    
    return(simu)
  }
  )
  
  simu <- sim %>% as.data.frame() %>% 
    setNames(gsub("geometry.", perl = TRUE, "", names(.))) %>%
    setNames(gsub("geometry", perl = TRUE, "0", names(.))) %>%
    tidyr::pivot_longer(., cols = everything(), names_to = "AnimalID", values_to = "geom") %>% 
    mutate(ID = as.numeric(AnimalID) + 1) %>%
    group_by(ID) %>%
    mutate(uid = seq_along(AnimalID)) #Assign sequential numbers to rows to preserve directionality
  
  simu <- simu %>%  
    mutate(POINT_X = sf::st_coordinates(geom)[,1], POINT_Y = sf::st_coordinates(geom)[,2]) %>%  #Obtain x and y coordinates so that you can force northward and southward movement
    dplyr::select(uid, 
                  ID, 
                  geom, POINT_Y, POINT_X) %>%
    group_by(ID) %>%
    arrange(uid) %>%
    mutate(time = seq.POSIXt(from = as.POSIXct("2000-01-01 01:00:00"), by = "1 hour", along.with = uid),   #Make up dates for the row numbers by Iteration and Animal ID
           start_x = POINT_X, start_y = POINT_Y, end_x = lead(POINT_X), end_y = lead(POINT_Y)) %>% #Prep data for making lines - ensures proper order
    sf::st_as_sf()
  
  #Make lines and unite geometries by uid's so that each line segment maintains time information
  simu <- simu %>%
    filter(!is.na(end_y)) %>%
    tidyr::nest() %>% 
    mutate(
      data = purrr::map(data, 
                        ~ mutate(.x,
                                 x = purrr::pmap(.l = list(start_x, start_y, end_x, end_y),
                                                 .f = make_line)))) 
  
  return(simu)
}

zero_var <- function(df) {
  
  # Extract zero variance variables first
  zero_var_rows <- df %>%
    group_by(res_name, gid) %>%
    summarise(sd = sd(dif)) %>%
    dplyr::filter(sd == 0) %>%
    dplyr::select(res_name, gid) %>%
    as.list()
  
  # Drop zero variance rows
  df <- df %>%
    group_by(res_name, gid) %>%
    dplyr::filter(!(gid %in% zero_var_rows$gid))
  
  return(df)
}

powr <- function(output, sig.level, power, delta) {
  stats::power.t.test(n = n, sd = max(na.omit(output$sd)), 
                      sig.level = sig.level, power = power, delta = delta,
                      type = "paired", alternative = "two.sided")
}

depth_data <- function(HSgrid) {
  
  alt <- sf::st_join(HSgrid, depth, join = sf::st_intersects) %>%
    group_by(gid) %>%
    summarise(mean_depth = mean(altitude)*-1)
  
  return(alt)
}

d_shore_rcvs <- function(km, study_site, land_barrier){
  
  grid_spacing <- km * 1000
  
  full_grid <- sf::st_make_grid(study_site, square = T, cellsize = c(grid_spacing, grid_spacing)) %>% # Create a grid within the bounding box of the study site
    st_as_sf %>% # Convert to sf object
    mutate(gid2 = seq_along(x)) # Create ID column for full grid

  grid_centroids <- grid_res(km, study_site, 3857, what = "centers") %>%  # Take the centroids of the grid
    st_as_sf() %>% # Convert to sf object
    mutate(d_shore = as.numeric(sf::st_distance(., land_barrier, by_element = TRUE))) # Find the distance to shore for each grid centroid
  
  # Correct coastal distances and renumber IDs
  grid_centroids <- grid_centroids %>%
    mutate(d_shore = if_else(sf::st_intersects(., land_barrier, sparse = F), 0, d_shore), # Grid centroids that instersect land have a shore distance of zero
           gid2 = seq_along(x)) %>% # Set an ID column that matches the full grid
    as.data.frame() %>%
    dplyr::select(-x)
  
  grid <- merge(grid_centroids, full_grid, by = "gid2") %>% # Merge the centroids with the full grid by the shared ID column
    st_as_sf() # Convert to sf object
  
  grid <- sf::st_intersection(grid, study_site) %>% # Clip the grid to the study site
    mutate(count = lengths(sf::st_intersects(., sts_pts)), # Count the number of receivers in each grid cell
           p_a = as.factor(if_else(count > 0, 1, 0)), # Indicate whether receivers are present or not
           gid = seq_along(gid2), # Re-number grid ID's to equal what the gid's would be if you created the grid inside the study site
           den_rcs = (count/sf::st_area(x))*1000000) # Calculate density of receivers per km^2
  
  return(grid)
}
```

## Get the objects required for simulating tracks

Get the study site, convert it to a sp object and set the projected coordinate system.

```{r}
#| label: study-site
#| include: true

study_site <- sf::st_read("fo_Bathy500m.shp") %>% 
  as(., 'Spatial') 

crs <- "+init=epsg:3857"

sp::proj4string(study_site) <- sp::CRS(crs)
```

Get the release site.

```{r}
#| label: release-site
#| include: true

rel_site <- sf::st_read("m_fo_rel_site.shp")
```

## Create the objects required for summarizing distribution information

Make the different resolution grid cells and put them all in a list.

```{r}
#| label: grids
#| include: true

HS_10km_grid <- grid_res(10, study_site)
HS_25km_grid <- grid_res(25, study_site)
HS_50km_grid <- grid_res(50, study_site)
HS_100km_grid <- grid_res(100, study_site)

HSgrid <- list(HS_100km_grid, HS_50km_grid, HS_25km_grid, HS_10km_grid)
```

## Get the objects required for deriving acoustic telemetry data

Get the stations.

```{r}
#| label: stations
#| include: true

stations <- sf::st_read("fo_stations.shp") 
```

## Get the objects required for re-routing tracks

Get the land barrier.

```{r}
#| label: land-barrier
#| include: true

land_barrier <- sf::st_read("fo_land_barrier.shp") %>%
  sf::st_transform(3857) %>%
  sf::st_collection_extract('POLYGON') %>%
  sf::st_cast('POLYGON')
```

Create a visibility graph from the land barrier object.

```{r}
#| label: visibility-graph
#| include: true

vis_graph <- pathroutr::prt_visgraph(land_barrier)
```

## Set the parameters values

For simulating tracks:

```{r}
#| label: track-params
#| include: true

# Number of animals
anims <- 30
# Number of years
yr <- 1       
# Turning angle - standard deviation of body flexion from blacktip kinematics study
theta <- c(0, 1.74)  
# Lower bound of mean recorded velocity for animal in m/s (ex: blacktip shark) (average - standard deviation)
vmin <- 0.98  
# Upper bound of mean recorded velocity for animal in m/s (ex: blacktip shark) (average + standard deviation)
vmax <- 1.58  
# Intersection of bounding box of release site(s) and study site
rel_site <- rel_site   
# Number of days animal is tracked
n_days <- 365*yr 
# Initial heading of animal
initHeading <- 0  
```

For the iterations:

```{r}
#| label: parallel-params
#| include: true

total_iterations <- 1000 # Total number of iterations desired
reps <- 1000 # Number of replicates that will result in desired iterations given reps
runs <- 1:(total_iterations/reps) # Interval at which outputs should be written to file
```

## Create a cluster

This will allow us to run these processes in parallel. Here, we find out how many cores we have, not including logical cores, and create a cluster

```{r}
#| label: create-cluster
#| include: true

numCores <- detectCores(logical = F)
numCores

cl <- makeCluster(numCores-4, outfile = "cluster_out")
```

Creating a cluster will create multiple instances of R on your computer. Now, we need to upload all of the libraries that each R instance will need to run the iterations. Notice that we have omitted the parallel library.

```{r}
#| label: cluster-libraries
#| include: true

clusterEvalQ(cl, {
  
  library(sf)
  library(sp)
  library(glatos)
  library(dplyr)
  library(momentuHMM)
  library(crawl)
  library(lubridate)
  library(pathroutr)
  library(sfnetworks)
  library(tidyverse)
  library(stplanr)
  library(future)
  library(doFuture)
  
})
```

Let's create an output file where our results will go. We only want to create this file once, otherwise we will overwrite our previous results. Thus, you will need to un-comment the lines to create it the first time. We want to assign the file name every time we run iterations, however.

```{r}
#| label: empty-list
#| include: true


# Create empty list object and save it
# df <- list()

# Assign the filename
filename <- "results_fixed-5.Rdata"

# Save the empty list object to the filename
# save(df, file = filename)
```

Let's remove any unnecessary objects we may have created before we load the environment up to the cluster

```{r}
#| label: clear-unneeded-objects
#| include: true

rm(list = c("HS_100km_grid", "HS_50km_grid", "HS_25km_grid", "HS_10km_grid"))
```

Now we're ready to mirror our global environment to the cluster cores.

```{r}
#| label: cluster-environment
#| include: true

parallel::clusterExport(cl, varlist = ls(envir = .GlobalEnv))
```

## Run the iterations

We should now have everything in place to iterate our methods. Here, we are using `base::replicate` to decide how often to save the iterations we have made. We are using `parallel::parLapply` to create the iterations. So, if we decided to use `reps <- 10` and `runs <- 100`, we would create 1,000 total iterations and save them every 100 iterations.

```{r}
#| label: iterate-methods
#| include: true

base::replicate(n = reps, expr = {
    
    # Clear cache
    gc()
    
    iterated_results <- parallel::parLapply(cl, runs, fun = function(I) {
      
      # Simulate tracks 
      df <- simul_trks(anims, study_site, theta, vmin, vmax, rel_site, n_days, initHeading)
      
      # Compare summary distribution info of simulated and modeled tracks across multiple grid cell resolutions
      df <- comp_trks(df, stations, land_barrier, vis_graph, HSgrid, multi.grid = TRUE, snap_tolerance = 650)
      
      return(df)
    }
    )
    # Load the previous results
    load(file = filename)
    # Append the new results to the previous results
    df <- append(df, iterated_results)
    # Overwrite the results with both the new and previous results combined
    save(df, file = filename)
    
    # Clear cache
    gc()
    
    return(df)
  }
  )
```

## Stop the cluster

You always want to stop the cluster when you are finished using it.

```{r}
#| label: stop-cluster
#| include: true

parallel::stopCluster(cl)
```

## Check the results

The number of iterations we want depends on how variable our results are so let's do a power analysis after we've run a few hundred iterations.

First, let's load the results.

```{r}
#| label: view-results
#| include: true

load(file = filename)
```

Hmm... these are hard to look at. Let's turn them into a data frame instead.

```{r}
#| label: flatten-results
#| include: true

# Bind the lists by grid cell resolution
df <- lapply(df, data.table::rbindlist, idcol = 'resolution')

# Bind the bound resolution by iteration
df <- data.table::rbindlist(df, idcol = 'iteration')
```

## Remove zero inflation

Remove any cases where both `sim_count` and `mod_count` were zero for all iterations within a grid resolution. First, let's merge grid resolution size names to the main data frame and calculating standard deviation by grid resolution.

```{r}
#| label: prep-power
#| include: true

stat <- df %>%
  left_join(tibble(resolution = 1:4, 
                   res_name = c("100km", "50km", "25km", "10km")), 
            by = "resolution") %>% 
  group_by(res_name, gid) %>%
  mutate(sd = sd(dif))
```

Test for zero variance.

```{r}
#| label: remove-zeroes
#| include: true

# Remove grid cells with zero variance
df_var <- zero_var(stat) 

# Find the grid cell with zero variance
df_zero_var <- anti_join(stat, df_var, by = c("res_name", "gid")) %>%
  distinct(res_name, gid)
```

### Prep for power analysis

Prep the results for a power analysis by setting some parameters for our power analysis. We previously set our number of animals, but it's likely that we've stepped away to work on other things before all these iterations finished so we can run it again here by un-commenting `anims <- 30` if we need to. Whatever variable you are interested in here, you set as `NULL`. So, if I want to know how many iterations I will need to achieve a power of 80% and an effect size within 1% of the total number of animals at a 95% significance level, I will leave `n <- NULL`.

```{r}
#| label: power-params
#| include: true

# The number of animals
anims <- 30

# The power that we want to achieve
power <- 0.8

# The effect size we want to achieve
delta <- anims*0.01 # a delta within 1% of the total number of animals

# The level of significance that we want to achieve
sig.level <- 0.95

# The number of iterations we will need to run to achieve the other parameters that we set
n <- NULL
```

The parameters `power,` `delta,` `sig.level`, and `n` will need to be chosen based on the study itself. In the past, this [statistics book](https://www.macmillanlearning.com/college/us/product/The-Analysis-of-Biological-Data/p/131922623X?searchText=whitlock) has served as a useful resource to me. It also comes with [R code resources](https://whitlockschluter3e.zoology.ubc.ca).

[![Useful statistics textbook.](https://prod-cat-files.macmillan.cloud/MediaResources/Jackets/258W/9781319226237.jpg){width="225"}](https://www.macmillanlearning.com/college/us/product/The-Analysis-of-Biological-Data/p/131922623X?searchText=whitlock)

We're ready to run our power analysis. We need to run one for each grid cell resolution. So, we'll do that by setting another variable called `res` and filtering for the grid cell resolution of interest in the results that we prepped for this power analysis.

```{r}
#| label: power-analysis
#| include: true

pow_stat <- stat %>%
  ungroup() %>%
  group_by(res_name)

res <- pow_stat %>% dplyr::filter(res_name == "100km")
pwr_100km <- powr(res, sig.level, power, delta)
print("Grid cell resolution: 100 km x 100 km")
pwr_100km

res <- pow_stat %>% dplyr::filter(res_name == "50km")
pwr_50km <- powr(res, sig.level, power, delta)
print("Grid cell resolution: 50 km x 50 km")
pwr_50km

res <- pow_stat %>% dplyr::filter(res_name == "25km")
pwr_25km <- powr(res, sig.level, power, delta)
print("Grid cell resolution: 25 km x 25 km")
pwr_25km


res <- pow_stat %>% dplyr::filter(res_name == "10km")
pwr_10km <- powr(res, sig.level, power, delta)
print("Grid cell resolution: 10 km x 10 km")
pwr_10km

```

## Sanity check

Do we really need THAT many iterations?! Let's do a sanity check here and see how variable our worst grid cell results were.

```{r}
#| label: sanity-check
#| include: true

max_sd <- pow_stat %>%
  ungroup() %>%
  group_by(res_name) %>%
  summarise(max_sd = max(sd))
max_sd
```

## Decisions, decisions

Here, we need to decide whether we are satisfied with the number of iterations we've run so far or if we need to run more iterations to achieve our desired power. If you want to run more iterations, you will return to the "[Create a cluster]" section. If you're satisfied, let's move on to the next section, [Satisfaction].

## Satisfaction

If you're satisfied with the results of the power analysis for your simulated acoustic detection data, let's summarize the differences between simulated and modeled tracks by grid cell within each grid resolution.

```{r}
#| label: results-summary
#| include: true

results <- pow_stat %>%
  group_by(res_name, gid) %>%
  summarise(avg_dif = mean(dif),
            se = plotrix::std.error(dif)) 
results
```

## Check the methods performance

Let's create a table that shows how closely the modeled tracks compared to the reconstructed tracks for each grid resolution. Here, we compare counts of grid cells within a certain difference from `sim_count` divided by the total number of grid cells.

```{r}
#| label: results-table
#| tbl-cap: Results of iterative methods
#| warning: false
#| include: true

how_well <- results %>%
  group_by(res_name) %>%
  mutate(maxgid = max(gid),
         res_name = ordered(res_name, levels = c("100km", "50km", "25km", "10km")),
         wn_0 = ifelse(avg_dif >= -1*0.1 & avg_dif <= 0.1, 1, 0),
         wn_1 = ifelse(avg_dif >= -1*1 & avg_dif <= 1, 1, 0), 
         wn_2 = ifelse(avg_dif >= -1*2 & avg_dif <= 2, 1, 0), 
         wn_3 = ifelse(avg_dif >= -1*3 & avg_dif <= 3, 1, 0), 
         wn_4 = ifelse(avg_dif >= -1*4 & avg_dif <= 4, 1, 0), 
         wn_5 = ifelse(avg_dif >= -1*5 & avg_dif <= 5, 1, 0)) %>%
  summarise(tot_wn_0 = sum(wn_0)/maxgid*100,
            tot_wn_1 = sum(wn_1)/maxgid*100,
            tot_wn_2 = sum(wn_2)/maxgid*100,
            tot_wn_3 = sum(wn_3)/maxgid*100,
            tot_wn_4 = sum(wn_4)/maxgid*100,
            tot_wn_5 = sum(wn_5)/maxgid*100) %>%
  distinct()
```

## Map results

Let's map these and see what kind of spatial trends we can see.

```{r}
#| label: results-maps
#| warning: false
#| include: true

mapview::mapView(results %>% dplyr::filter(res_name == "100km"))
mapview::mapView(results %>% dplyr::filter(res_name == "50km"))
mapview::mapView(results %>% dplyr::filter(res_name == "25km"))
mapview::mapView(results %>% dplyr::filter(res_name == "10km"))
```

We had a problem in the track reconstruction where some iterations did not reconstruct tracks for all `anims`. Let's find out how many track reconstructions were successful across all the iterations for each grid cell resolution.

```{r}
#| label: recon-info
#| warning: false
#| include: true

recon <- stat %>%
  group_by(res_name, iteration) %>%
  summarise(max_recon = max(mod_count)) %>%
  group_by(res_name) %>%
  summarise(max_30 = length(which(max_recon >= 30))/max(iteration)*100,
            max_29 = length(which(max_recon >= 29))/max(iteration)*100,
            max_28 = length(which(max_recon >= 28))/max(iteration)*100,
            max_27 = length(which(max_recon >= 27))/max(iteration)*100,
            max_26 = length(which(max_recon >= 26))/max(iteration)*100,
            max_25 = length(which(max_recon >= 25))/max(iteration)*100)
  
```

## Gather more information

In this section, we want to find out why the results are the way they are. Can we derive some information about the grid cells that tell us what contributes to a better result?\

First, let's gather information on the depth in each grid cell. For this example, I have logged onto the National Oceanic and Atmospheric Administration (NOAA) [ERDDAP server](https://coastwatch.pfeg.noaa.gov/erddap/search/advanced.html?page=1&itemsPerPage=1000) and gathered depth information for the bounding box of the study site, which must be in a geographic coordinate system (e.g., NAD 1983). Let's get the bounding box of our study site.[^1]

[^1]: Note: Some acoustic telemetry users along the U.S. Eastern Seaboard have experienced theft of their equipment and so they are hesitant to share exact locations of receivers. For that reason, the real receiver locations in this vignette have been obscured with a false origin and the related shapefiles were altered in kind to visually match the obscured receiver locations. However, to get the actual environmental information required to make meaningful inferences about these results, we need to pull information from the actual study site. Therefore, we call in the actual study site for this section.

```{r}
#| label: get-bbox
#| include: true

# Objects required for iterations to run
site_depth <- sf::st_read("Bathy500m.shp") %>%
  sf::st_transform(4269)

sf::st_bbox(site_depth)
```

We will use the `xmin` (Min Longitude), `ymin` (Min Latitude), `xmax` (Max Longitude), and `ymax` (Max Latitude) to search for relevant depth (altitude) data in the ERDDAP server.

![NOAA ERDDAP server advanced search parameters for depth measurements in study site.](images/Screen%20Shot%202023-06-12%20at%204.24.59%20PM.png)

We are going to use the `DatasetID: etopo180`

![](images/Screen%20Shot%202023-06-12%20at%204.27.51%20PM.png)

![ERDDAP search results](images/Screen%20Shot%202023-06-12%20at%204.28.00%20PM.png){width="229"}

Click on `graph` and then click on `Download the Data or an Image`

![How to get the ERDDAP data](images/Screen%20Shot%202023-06-12%20at%204.30.46%20PM.png)

Save the export with filename "DepthData.csv"

```{r}
#| label: gather-depth
#| include: true

atlcoast <- sf::st_read("atlcoast.shp") %>%
  sf::st_transform(4269)
```

Read in depth data. Here, we also filter the depth data to only be those depths found inside the study site - right now, it's everything inside the grid bounding box.

```{r}
#| label: depth-data
#| warning: false
#| include: true

depth <- readr::read_csv(file = "Depth_Data.csv") %>%
  sf::st_as_sf(coords = c("longitude", "latitude"), crs = 4269) 

depth <- sf::st_filter(depth, site_depth) %>% 
  mutate(uid = seq_along(geometry)) 
```

We also need to re-create the grids without the false origin.[^2] The study site must be in a projected coordinate system (WGS 84; EPSG: 3857) when it is initially fed into the function `grid_res`.

[^2]: Note: Some acoustic telemetry users along the U.S. Eastern Seaboard have experienced theft of their equipment and so they are hesitant to share exact locations of receivers. For that reason, the real receiver locations in this vignette have been obscured with a false origin and the related shapefiles were altered in kind to visually match the obscured receiver locations. However, to get the actual environmental information required to make meaningful inferences about these results, we need to pull information from the actual study site. Therefore, we call in the actual study site for this section.

```{r}
#| label: true-grids
#| warning: false
#| include: true

study_site <- site_depth %>%
  sf::st_transform(., 3857)

HS_100km_grid <- grid_res(100, study_site, 4269, "polygons")
HS_50km_grid <- grid_res(50, study_site, 4269, "polygons")
HS_25km_grid <- grid_res(25, study_site, 4269, "polygons")
HS_10km_grid <- grid_res(10, study_site, 4269, "polygons")
```

Merge depth data with different grid cell resolutions.

```{r}
#| label: true-depth
#| warning: false
#| include: true

HS_100km_grid <- depth_data(HS_100km_grid)
HS_50km_grid <- depth_data(HS_50km_grid)
HS_25km_grid <- depth_data(HS_25km_grid)
HS_10km_grid <- depth_data(HS_10km_grid)
```

Calculate distance to shore, density of receivers, presence/absence of receivers, and merge with grid cell depth data. We can do all of this using the false origin shapefiles.

```{r}
#| label: other-variables
#| warning: false
#| include: true

grid_100km <- merge((HS_100km_grid %>% as.data.frame() %>% dplyr::select(-geometry)), d_shore_rcvs(100, study_site, land_barrier), by = "gid") %>%
  sf::st_as_sf()

grid_50km <- merge((HS_50km_grid %>% as.data.frame %>% dplyr::select(-geometry)), d_shore_rcvs(50, study_site, land_barrier), by = "gid") %>%
  sf::st_as_sf()

grid_25km <- merge((HS_25km_grid %>% as.data.frame %>% dplyr::select(-geometry)), d_shore_rcvs(25, study_site, land_barrier), by = "gid") %>%
  sf::st_as_sf()

grid_10km <- merge((HS_10km_grid %>% as.data.frame %>% dplyr::select(-geometry)), d_shore_rcvs(10, study_site, land_barrier), by = "gid") %>%
  sf::st_as_sf()


```

Let's find out how many grid cells lacked receivers.

```{r}
#| label: rcv-absence
#| warning: false
#| include: true

grids <- list(grid_100km, grid_50km, grid_25km, grid_10km)

grids <- sf::st_as_sf(data.table::rbindlist(grids, idcol = 'resolution')) %>%
  left_join(tibble(resolution = 1:4, res_name = c("100km", "50km", "25km", "10km")), by = "resolution") %>%
  mutate(res_name = ordered(res_name, levels = c("100km", "50km", "25km", "10km")))

no_rcv <- grids %>%
  group_by(res_name) %>%
  summarise(n_zeroes = sum(p_a == "0"),
            perc_zeroes = n_zeroes/max(gid)*100) %>%
  as.data.frame %>%
  dplyr::select(-x)
```

Merge the results to these grid cell metadata.

```{r}
#| label: results-vars
#| warning: false
#| include: true

results <- merge(df, grids, by = c("res_name", "gid")) %>%
  sf::st_as_sf()
```

# Model the effects

We want to create a model to understand which variables affect the fit in the grid cells.

## Create a dummy model

We need to check for colinearity between variables to see if we need to include any interaction terms in the model.

```{r}
#| label: vif
#| warning: false
#| include: true

dummy.fit <- glm(formula = dif ~ den_rcs + p_a + mean_depth + d_shore, family = gaussian(), data = grids %>% dplyr::select(res))

library(car)
car::vif(dummy.fit)
```
